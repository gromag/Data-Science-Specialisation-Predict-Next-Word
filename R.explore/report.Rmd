---
title: "Capstone project Data Analysis - Natural Language Processing"
author: "Giuseppe Romagnuolo"
date: "19 March 2016"
output: html_document
---

##Synopsis

In this report I'm going to do some Exploratory Data Analysis of 3 Corpora from [http://www.corpora.heliohost.org/][heliohost], the files are a collection of Blog posts, `en_US.blogs.txt`, a collection of twitter posts `en_US.twitter.txt` and a collection of news documents `en_Us.news.txt`.

[heliohost]:http://www.corpora.heliohost.org/

##Exploratory analysis

###Loading the data

The 3 corpora are quite hefty in size, the `en_US.blogs.txt` file alone has almost `900,000` documents and a total of nearly `4,000,000` words. Analysing such big corpus is rather demanding on the CPU and RAM. For an exploratory analysis I will use a sample from the 3 corpora.


Let's sample roughly 5% of the lines from the given files. 

_(Note, in preparation of the final project I have started organising functions into their own separate files, here therefore I'm loading all `.R` files from the `/R` subdirectory using the command `sapply(list.files(pattern = "[.]R$", path="R", full.names = T), source)`. I'm omitting the output here for a mere document formatting reason please refer to the `Appendix, Loaded Functions` for the source code of the loaded functions.)_


```{r include=FALSE}
#Loading external source files containing all functions needed here
sapply(list.files(pattern = "[.]R$", path="R", full.names = T), source)

```

```{r cache=TRUE, warning=FALSE}

seed <- 123

blogLines <- sampleLines(0.05, "data/raw/final/en_US/en_US.blogs.txt", seed)
newsLines <- sampleLines(0.05, "data/raw/final/en_US/en_US.news.txt", seed)
twitterLines <- sampleLines(0.05, "data/raw/final/en_US/en_US.twitter.txt", seed)

```

Let's get a basic summary of how many documents we are dealing with and how many words there are per corpus.

```{r}

comparisonStats(blogLines, newsLines, twitterLines)


```


###Cleaning

The data comes with lots of non words characters like punctuation, emoticons etc that can be removed for the purpose of our analysis. Also we want to clean the corpora from any profane words. I have downloaded a free to use list of profane words from [http://www.bannedwordlist.com/][bannedwords] and saved in a text file which I'm going to use as my profanity dictionary.

```{r cache=T}
profaneWords <- getProfaneWords("data/swearWords.csv")

```

```{r cache=T}

blogLines.clean <- cleanText(removeProfaneWords(blogLines, profaneWords))
newsLines.clean <- cleanText(removeProfaneWords(newsLines, profaneWords))
twitterLines.clean <- cleanText(removeProfaneWords(twitterLines, profaneWords))

```

Let's run the comparison summary, we should see that the words counts have lowered for all 3 corpora.

```{r}
comparisonStats(blogLines.clean, newsLines.clean, twitterLines.clean)

```
[bannedwords]: http://www.bannedwordlist.com/


R has various packages to help with Natural Language Processing, one of the core ones is the `tm` package which comes with a `Corpus` function and various transformation function that help with cleaning the documents, we could have used the `tm::removeWords` transformation function to remove profane words, we are going to use it instead to clean up the documents of common words of the English dictionary like 'and', 'but', 'the' which add no value to our analysis.

```{r warning= FALSE}
library(tm)

getTransformations()

```

Let's load the documents into `Corpus` objects.

```{r cache=T}
blogCorpus <- getCorpus(blogLines.clean)
newsCorpus <- getCorpus(newsLines.clean)
twitterCorpus <- getCorpus(twitterLines.clean)

```

###Analysis

We are going to use RWeka to help us with tokenisation, first we transform our `Corpora` into `TermDocumentMatrixes` so that we can run some stats about the frequency of words.

```{r warning=F}
library(RWeka)
```

```{r cache=T }

blogTdm <- getTermDocumentMatrix(blogCorpus, 1)
newsTdm <- getTermDocumentMatrix(newsCorpus, 1)
twitterTdm <- getTermDocumentMatrix(twitterCorpus, 1)

```

``` {r}

blogTdmNoSparse <- removeSparseTerms(blogTdm, 0.99 )

newsTdmNoSparse <- removeSparseTerms(newsTdm, 0.99)

twitterTdmNoSparse <- removeSparseTerms(twitterTdm, 0.99)



```

``` {r}
blogTdm.matrix <- as.matrix(blogTdmNoSparse)

newsTdm.matrix <- as.matrix(newsTdmNoSparse)

twitterTdm.matrix <- as.matrix(twitterTdmNoSparse)

blogTdm.summary <- apply(blogTdm.matrix, MARGIN = 1, sum)

newsTdm.summary <- apply(newsTdm.matrix, MARGIN = 1, sum)

twitterTdm.summary <- apply(twitterTdm.matrix, MARGIN = 1, sum)

```

Lets find out the most and least frequent words in the Blog, News and Twitter corpora.

```{r}

topBlog <- head(sort(blogTdm.summary, decreasing = T))
 
leastBlog <- head(sort(blogTdm.summary, decreasing = F))
        
        
barplot(topBlog, xlab = "Words", ylab = "Frequency", main = "Top Blog Frequent Words" )   

barplot(leastBlog, xlab = "Words", ylab = "Frequency", main = "Least Blog Frequent Words" ) 



topNews <- head(sort(newsTdm.summary, decreasing = T))
 
leastNews <- head(sort(newsTdm.summary, decreasing = F))
        
        
barplot(topNews, xlab = "Words", ylab = "Frequency", main = "Top News Frequent Words" )   

barplot(leastNews, xlab = "Words", ylab = "Frequency", main = "Least News Frequent Words" ) 


topTwitter <- head(sort(twitterTdm.summary, decreasing = T))
 
leastTwitter <- head(sort(twitterTdm.summary, decreasing = F))
        
        
barplot(topTwitter, xlab = "Words", ylab = "Frequency", main = "Top Twitter Frequent Words" )   

barplot(leastTwitter, xlab = "Words", ylab = "Frequency", main = "Least Twitter Frequent Words" ) 


```

From the above I can see that many very common words are still part of the document and a more in depth analysis would not focus on those.

The project will need me to focus on bigrams and trigrams (sequence of two and three words). The gist of the Shiny application would need to calculate the probability of a new word given a 'history' of words that were inserted before.

This will be the next step going forward towards the Capstone Project.

##Appendix

###Loaded Functions

Below is the source code of the externally loaded files.

```{r cache=F}
#Loading external source files containing all functions needed here
sapply(list.files(pattern = "[.]R$", path="R", full.names = T), source)

```


##Disclaimer
This analysis was done as part of the Capstone Project course which is part of the [Data Science Specialisation][dss] provided by [Johns Hopkins Bloomberg School of Public Health][jhsph] via [Coursera][co].


[dss]: https://www.coursera.org/specialization/jhudatascience/1
[jhsph]: http://www.jhsph.edu/
[co]: https://www.coursera.org/
[mtdesc]:https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html